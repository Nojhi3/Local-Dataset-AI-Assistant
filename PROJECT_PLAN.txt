Local AI Chatbot Demo Project Plan
Date: February 26, 2026
Target: 1-week delivery

Objective
- Build a local AI chatbot that answers questions only from uploaded CSV/JSON datasets.
- Use Streamlit frontend, Python backend, local DB, and Ollama-powered RAG.
- Containerize frontend and backend and run everything locally with Docker Compose.

Scope
- Frontend: file upload, dataset selection, chat window, response display.
- Backend: ingestion API, chat API, local storage, retrieval + LLM response.
- Data store: SQLite for metadata/records.
- Retrieval: local embeddings + vector store (Chroma).
- LLM: Ollama local model.

Architecture (Implemented)
1. Streamlit UI uploads file to backend.
2. Backend validates and parses CSV/JSON.
3. Parsed records are stored in SQLite.
4. Chat requests retrieve relevant rows using lexical scoring and threshold guardrails.
5. Retrieved context is passed to Ollama for grounded answer generation.
6. Prompt policy enforces "answer from ingested data only".

Milestones Status
Completed:
- Scaffold project structure.
- Add health-check endpoints and basic Streamlit app.
- Implement CSV/JSON parser and validation.
- Persist uploaded data and metadata in SQLite.
- Implement retrieval-based answer generation via Ollama.
- Integrate Streamlit upload and chat with backend APIs.
- Add strict "no relevant context => I don't know" behavior.
- Add tests for upload, invalid format, and chat-without-dataset.
- Add smoke test and backend logging.
- Create internal implementation notes and debugging playbook.

In Progress:
- Docker end-to-end validation run and model pull verification.

Pending:
- Final submission polish (README/demo steps/screenshots).

Definition of Done
- User can upload CSV/JSON from UI.
- Uploaded data is stored in SQLite locally.
- Chat responses are grounded in uploaded data.
- Out-of-scope questions return "not found in provided data" style response.
- Stack runs using docker compose without cloud services.

Risks and Mitigations
- Model mismatch (wrong model name):
  - Standardize on `llama3.2:3b`.
- Wrong Ollama host between local and Docker:
  - Local: `http://127.0.0.1:11434`
  - Docker: `http://ollama:11434`
- Hallucinations:
  - Strict prompt and retrieval threshold fallback to "I don't know".

Next Immediate Build Tasks
1. Complete Docker E2E validation on laptop.
2. Capture final demo script + screenshots.
3. Optional: upgrade retrieval from lexical to vector-based semantic search.
